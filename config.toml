[training]
mode = "debug" # debug or full
model_config = "B4"  # Options: B4, B2, M2, L2, XL2, XL2_plus
condition_type = "t_r"  # Options: t_r, t_delta_t

input_size = 32
batch_size = 64
n_epochs = 80  # Number of epochs to train
num_workers = 0

learning_rate = 0.0001
weight_decay = 0.00

gamma = 0.0 # p = 1 - gamma
c = 0.01
sample_ratio = 0.25
sampler_type = "lognorm"

# Classifier-Free Guidance (CFG) settings
cfg_ratio = 0.1           # Ratio of samples to apply CFG (0.0 = disabled, 0.1 = 10% of batch)
cfg_scale = 2.0           # CFG scale/strength (0.0 = no guidance)

# Training intervals (in steps)
log_step = 500                # Log metrics every N steps
sample_every_n_epochs = 5     # Generate samples every N epochs
checkpoint_every_n_epochs = 5 # Save checkpoint every N epochs
histogram_step = 2000         # Log histograms every N steps

# Training settings
mixed_precision = "bf16"      # Mixed precision mode: fp16, bf16, or no (bf16 is more stable than fp16)
jvp_use_autograd = true       # Use autograd for JVP (true) or funtorch (false)

# Sampling settings
n_samples_per_class = 1       # Number of samples per class during evaluation
sampling_steps = 5            # Number of steps for sample generation
sample_grid_nrow = 10         # Number of columns in sample image grid

[training.sample_params]
sigma = 1.0
mean = -0.4

# Model architectures
# depth: number of transformer layers
# hidden_dim: hidden dimension size
# num_heads: number of attention heads
# patch_size: patch size for image tokenization
[model.B4]
depth = 12
hidden_dim = 768
num_heads = 12
patch_size = 4

[model.B2]
depth = 12
hidden_dim = 768
num_heads = 12
patch_size = 2

[model.M2]
depth = 16
hidden_dim = 1024
num_heads = 16
patch_size = 2

[model.L2]
depth = 24
hidden_dim = 1024
num_heads = 16
patch_size = 2

[model.XL2]
depth = 28
hidden_dim = 1152
num_heads = 16
patch_size = 2

[model.XL2_plus]
depth = 28
hidden_dim = 1152
num_heads = 16
patch_size = 2
